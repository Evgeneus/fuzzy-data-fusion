{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy.random import beta\n",
    "import time\n",
    "\n",
    "# to compute exp of each cell in a matrix\n",
    "exp = np.vectorize(math.exp)\n",
    "\n",
    "\n",
    "def MV(Psi):\n",
    "    \"\"\"\n",
    "    The majority voting method.\n",
    "    \"\"\"\n",
    "    return [np.bincount([y[1] for y in x]) / (0.0 + len(x)) for x in Psi]\n",
    "\n",
    "\n",
    "def EM(N, M, Psi, inv_Psi):\n",
    "    \"\"\"\n",
    "    The expectation maximization method (EM) from Dong et al., 2013.\n",
    "    \"\"\"\n",
    "    # convergence eps\n",
    "    eps = 0.0001\n",
    "\n",
    "    # init accuracies\n",
    "    A = [0.8]*N\n",
    "    while True:\n",
    "        # E-step\n",
    "        p = [[0, 0] for x in range(M)]\n",
    "        for obj in range(M):\n",
    "            C = [0]*2\n",
    "            for s, val in Psi[obj]:\n",
    "                C[val] += math.log(A[s])\n",
    "                C[not val] += math.log(1-A[s])\n",
    "            p[obj][1] = math.exp(C[1])/(math.exp(C[0])+math.exp(C[1]))\n",
    "            p[obj][0] = math.exp(C[0]) / (math.exp(C[0]) + math.exp(C[1]))\n",
    "\n",
    "        # M-step\n",
    "        A_new = [np.average([p[y[0]][y[1]] for y in x]) for x in inv_Psi]\n",
    "\n",
    "        # convergence check\n",
    "        if sum(abs(np.subtract(A, A_new))) < eps:\n",
    "            A = A_new\n",
    "            break\n",
    "        else:\n",
    "            A = A_new\n",
    "\n",
    "    return A, p\n",
    "\n",
    "\n",
    "def log_likelihood(Psi, A, p):\n",
    "    \"\"\"\n",
    "    Computes the log likelihood of the Psi using A and p.\n",
    "    \"\"\"\n",
    "    res = 0\n",
    "    for obj_id in range(M):\n",
    "        for source_id, value_id in Psi[obj_id]:\n",
    "            if value_id == 1:\n",
    "                res += math.log(A[source_id] * p[obj_id][value_id])\n",
    "            else:\n",
    "                res += math.log((1 - A[source_id]) * (1 - p[obj_id][value_id]))\n",
    "    return res\n",
    "\n",
    "\n",
    "def random_log_likelihood(N, M, Psi):\n",
    "    \"\"\"\n",
    "    Searches for the max log likelihood at random.\n",
    "    \"\"\"\n",
    "    # number of attempts\n",
    "    N_iter = 10\n",
    "\n",
    "    max_log_likelihood = -10000000.0\n",
    "    bf_A = []\n",
    "    bf_p = []\n",
    "    for i in range(N_iter):\n",
    "        A = np.random.uniform(0.8, 1.0, N)\n",
    "        p = [[1 - x, x] for x in np.random.uniform(0, 1, M)]\n",
    "        cur_ll = log_likelihood(Psi, A, p)\n",
    "        if cur_ll > max_log_likelihood:\n",
    "            max_log_likelihood = cur_ll\n",
    "            bf_A = A\n",
    "            bf_p = p\n",
    "\n",
    "    return bf_A, bf_p\n",
    "\n",
    "\n",
    "def mcmc(N, M, Psi, inv_Psi):\n",
    "    \"\"\"\n",
    "    MCMC for log-likelihood maximum search.\n",
    "    \"\"\"\n",
    "    N_iter = 15\n",
    "    burnin = 5\n",
    "    thin = 2\n",
    "\n",
    "    # random init\n",
    "    A = np.random.uniform(0.8, 1.0, N)\n",
    "\n",
    "    # MCMC sampling\n",
    "    sample_size = 0.0\n",
    "    mcmc_p = [[0.0, 0.0] for x in range(M)]\n",
    "    for _iter in range(N_iter):\n",
    "        # update objects\n",
    "        p = [[0.0, 0.0] for x in range(M)]\n",
    "        for obj in range(M):\n",
    "            C = [0.0]*2\n",
    "            for s, val in Psi[obj]:\n",
    "                C[val] += math.log(A[s])\n",
    "                C[not val] += math.log(1-A[s])\n",
    "            p[obj][1] = math.exp(C[1])/(math.exp(C[0])+math.exp(C[1]))\n",
    "            p[obj][0] = math.exp(C[0]) / (math.exp(C[0]) + math.exp(C[1]))\n",
    "        O = [1 if np.random.rand() < p[i][1] else 0 for i in range(M)]\n",
    "\n",
    "        # update sources\n",
    "        for source_id in range(N):\n",
    "            beta_0 = 0\n",
    "            beta_1 = 0\n",
    "            for obj, val in inv_Psi[source_id]:\n",
    "                if val == O[obj]:\n",
    "                    beta_0 += 1\n",
    "                else:\n",
    "                    beta_1 += 1\n",
    "            A[source_id] = beta(beta_0 + 4, beta_1 + 1)\n",
    "\n",
    "        #print(A, O)\n",
    "        if _iter > burnin and _iter % thin == 0:\n",
    "            sample_size += 1\n",
    "            for obj in range(M):\n",
    "                mcmc_p[obj][O[obj]] += 1\n",
    "\n",
    "    # mcmc output\n",
    "    # fix: if there are zero counts change them to tiny amounts\n",
    "    for p in mcmc_p:\n",
    "        if p[0] == 0:\n",
    "            p[0] += 0.0001\n",
    "            p[1] -= 0.0001\n",
    "        elif p[1] == 0:\n",
    "            p[0] -= 0.0001\n",
    "            p[1] += 0.0001\n",
    "    mcmc_p = [[p[0]/sample_size, p[1]/sample_size] for p in mcmc_p]\n",
    "    mcmc_A = [0]*N\n",
    "    for s in range(N):\n",
    "        for obj, val in inv_Psi[s]:\n",
    "            # TODO take advantage of priors (as in Zhao et al., 2012)\n",
    "            mcmc_A[s] += mcmc_p[obj][val]\n",
    "        mcmc_A[s] /= (0.0+len(inv_Psi[s]))\n",
    "\n",
    "    return mcmc_A, mcmc_p\n",
    "\n",
    "# number of sources\n",
    "N = 20\n",
    "# number of objects\n",
    "M = 1000000\n",
    "# synthetically generated observations\n",
    "density = 0.7\n",
    "accuracy = 0.8\n",
    "Psi = [[(s, int(np.random.rand() < density)) for s in range(N) if np.random.rand() < accuracy] for obj in range(M)]\n",
    "# compute inverted Psi (indexing)\n",
    "inv_Psi = [[] for s in range(N)]\n",
    "for obj in range(M):\n",
    "    for s, val in Psi[obj]:\n",
    "        inv_Psi[s].append( (obj, val) )\n",
    "\n",
    "start = time.time()\n",
    "mv_p = MV(Psi)\n",
    "mv_t = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "em_A, em_p = EM(N, M, Psi, inv_Psi)\n",
    "em_t = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "bf_A, bf_p = random_log_likelihood(N, M, Psi)\n",
    "bf_t = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "mcmc_A, mcmc_p = mcmc(N, M, Psi, inv_Psi)\n",
    "mcmc_t = time.time() - start\n",
    "\n",
    "print('MV   Pr: {:1.3f}\\tlog-likelihood: N/A  \\ttime: {:5.1f}'.format(np.average([x[1] for x in mv_p]), mv_t))\n",
    "print('EM   Pr: {:1.3f}\\tlog-likelihood: {:3.2f}\\ttime: {:5.1f}'.format(np.average([x[1] for x in em_p]), log_likelihood(Psi, em_A, em_p), em_t))\n",
    "print('RND  Pr: {:1.3f}\\tlog-likelihood: {:3.2f}\\ttime: {:5.1f}'.format(np.average([x[1] for x in bf_p]), log_likelihood(Psi, bf_A, bf_p), bf_t))\n",
    "print('MCMC Pr: {:1.3f}\\tlog-likelihood: {:3.2f}\\ttime: {:5.1f}'.format(np.average([x[1] for x in mcmc_p]), log_likelihood(Psi, mcmc_A, mcmc_p), mcmc_t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
