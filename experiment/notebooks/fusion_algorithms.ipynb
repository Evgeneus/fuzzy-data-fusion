{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 33)\n",
      "MV   Pr: 0.944\n",
      "EM   Pr: 1.000\tlog-likelihood: -2.70\tA: [0.83333333336386961, 0.99999999995419697, 0.99999999995419697]\tp: [[2.290184421798629e-10, 0.9999999997709815], [9.16013573199207e-12, 0.9999999999908399], [9.16013573199207e-12, 0.9999999999908399], [9.16013573199207e-12, 0.9999999999908399], [9.16013573199207e-12, 0.9999999999908399], [9.16013573199207e-12, 0.9999999999908399]]\n",
      "RND  Pr: 0.870\tlog-likelihood: -5.99\tA: [ 0.87438904  0.96585112  0.94824855]\tp: [[0.38175117395775393, 0.61824882604224607], [0.036643436402551521, 0.96335656359744848], [0.0066079709528127362, 0.99339202904718726], [0.13190427518325654, 0.86809572481674346], [0.13348021537284793, 0.86651978462715207], [0.090469234471765136, 0.90953076552823486]]\n",
      "MCMC Pr: 0.657\tlog-likelihood: -15.98\tA: [0.6212121212121208, 0.6565656565656561, 0.6565656565656561]\tp: [[0.3939393939393938, 0.6060606060606057], [0.33333333333333326, 0.6666666666666663], [0.33333333333333326, 0.6666666666666663], [0.33333333333333326, 0.6666666666666663], [0.33333333333333326, 0.6666666666666663], [0.33333333333333326, 0.6666666666666663]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy.random import beta\n",
    "\n",
    "# to compute exp of each cell in a matrix\n",
    "exp = np.vectorize(math.exp)\n",
    "\n",
    "\n",
    "def MV(Psi):\n",
    "    \"\"\"\n",
    "    The majority voting method.\n",
    "    \"\"\"\n",
    "    return [np.bincount([y[1] for y in x]) / (0.0 + len(x)) for x in Psi]\n",
    "\n",
    "\n",
    "def EM(N, M, Psi, inv_Psi):\n",
    "    \"\"\"\n",
    "    The expectation maximization method (EM) from Dong et al., 2013.\n",
    "    \"\"\"\n",
    "    # convergence eps\n",
    "    eps = 0.0001\n",
    "\n",
    "    # init accuracies\n",
    "    A = [0.8]*N\n",
    "    while True:\n",
    "        # E-step\n",
    "        p = [[0, 0] for x in range(M)]\n",
    "        for obj in range(M):\n",
    "            C = [0]*2\n",
    "            for s, val in Psi[obj]:\n",
    "                C[val] += math.log(A[s])\n",
    "                C[not val] += math.log(1-A[s])\n",
    "            p[obj][1] = math.exp(C[1])/(math.exp(C[0])+math.exp(C[1]))\n",
    "            p[obj][0] = math.exp(C[0]) / (math.exp(C[0]) + math.exp(C[1]))\n",
    "\n",
    "        # M-step\n",
    "        A_new = [np.average([p[y[0]][y[1]] for y in x]) for x in inv_Psi]\n",
    "\n",
    "        # convergence check\n",
    "        if sum(abs(np.subtract(A, A_new))) < eps:\n",
    "            A = A_new\n",
    "            break\n",
    "        else:\n",
    "            A = A_new\n",
    "\n",
    "    return A, p\n",
    "\n",
    "\n",
    "def log_likelihood(Psi, A, p):\n",
    "    \"\"\"\n",
    "    Computes the log likelihood of the Psi using A and p.\n",
    "    \"\"\"\n",
    "    res = 0\n",
    "    for obj_id in range(M):\n",
    "        for source_id, value_id in Psi[obj_id]:\n",
    "            if value_id == 1:\n",
    "                res += math.log(A[source_id] * p[obj_id][value_id])\n",
    "            else:\n",
    "                res += math.log((1 - A[source_id]) * (1 - p[obj_id][value_id]))\n",
    "    return res\n",
    "\n",
    "\n",
    "def random_log_likelihood(N, M, Psi):\n",
    "    \"\"\"\n",
    "    Searches for the max log likelihood at random.\n",
    "    \"\"\"\n",
    "    # number of attempts\n",
    "    N_iter = 10000\n",
    "\n",
    "    max_log_likelihood = -100\n",
    "    bf_A = []\n",
    "    bf_p = []\n",
    "    for i in range(N_iter):\n",
    "        A = np.random.uniform(0.8, 1.0, N)\n",
    "        p = [[1 - x, x] for x in np.random.uniform(0, 1, M)]\n",
    "        cur_ll = log_likelihood(Psi, A, p)\n",
    "        if cur_ll > max_log_likelihood:\n",
    "            max_log_likelihood = cur_ll\n",
    "            bf_A = A\n",
    "            bf_p = p\n",
    "\n",
    "    return bf_A, bf_p\n",
    "\n",
    "\n",
    "def mcmc(N, M, Psi, inv_Psi):\n",
    "    \"\"\"\n",
    "    MCMC for log-likelihood maximum search.\n",
    "    \"\"\"\n",
    "    N_iter = 1000\n",
    "    burnin = 10\n",
    "    thin = 30\n",
    "    sample_size = (N_iter-burnin)/thin\n",
    "\n",
    "    # random init\n",
    "    A = np.random.uniform(0.8, 1.0, N)\n",
    "\n",
    "    # MCMC sampling\n",
    "    test_sample_size = 0\n",
    "    mcmc_p = [[0, 0] for x in range(M)]\n",
    "    for _iter in range(N_iter):\n",
    "        # update objects\n",
    "        p = [[0, 0] for x in range(M)]\n",
    "        for obj in range(M):\n",
    "            C = [0]*2\n",
    "            for s, val in Psi[obj]:\n",
    "                C[val] += math.log(A[s])\n",
    "                C[not val] += math.log(1-A[s])\n",
    "            p[obj][1] = math.exp(C[1])/(math.exp(C[0])+math.exp(C[1]))\n",
    "            p[obj][0] = math.exp(C[0]) / (math.exp(C[0]) + math.exp(C[1]))\n",
    "        O = [1 if np.random.rand() < p[i][1] else 0 for i in range(M)]\n",
    "\n",
    "        # update sources\n",
    "        for source_id in range(N):\n",
    "            beta_0 = 0\n",
    "            beta_1 = 0\n",
    "            for obj, val in inv_Psi[source_id]:\n",
    "                if val == O[obj]:\n",
    "                    beta_0 += 1\n",
    "                else:\n",
    "                    beta_1 += 1\n",
    "            A[source_id] = beta(beta_0 + 1, beta_1 + 1)\n",
    "\n",
    "        if _iter > burnin and _iter % thin == 0:\n",
    "            test_sample_size += 1\n",
    "            for obj in range(M):\n",
    "                mcmc_p[obj][O[obj]] += 1/(0.0+sample_size)\n",
    "\n",
    "    # mcmc output\n",
    "    mcmc_A = [0]*N\n",
    "    for s in range(N):\n",
    "        for obj, val in inv_Psi[s]:\n",
    "            # TODO take advantage of priors (as in Zhao et al., 2012)\n",
    "            mcmc_A[s] += mcmc_p[obj][val]\n",
    "        mcmc_A[s] /= (0.0+len(inv_Psi[s]))\n",
    "\n",
    "    return mcmc_A, mcmc_p\n",
    "\n",
    "\n",
    "# number of sources\n",
    "N = 3\n",
    "# number of objects\n",
    "M = 6\n",
    "# observations\n",
    "# chain\n",
    "Psi = [[(0, 0), (1, 1), (2, 1)],\n",
    "       [(0, 1), (1, 1), (2, 1)],\n",
    "       [(0, 1), (1, 1), (2, 1)],\n",
    "       [(0, 1), (1, 1), (2, 1)],\n",
    "       [(0, 1), (1, 1), (2, 1)],\n",
    "       [(0, 1), (1, 1), (2, 1)]]\n",
    "\n",
    "# inverted observations\n",
    "inv_Psi = [[(0, 0), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)],\n",
    "           [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)],\n",
    "           [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]]\n",
    "\n",
    "mv_p = MV(Psi)\n",
    "em_A, em_p = EM(N, M, Psi, inv_Psi)\n",
    "bf_A, bf_p = random_log_likelihood(N, M, Psi)\n",
    "mcmc_A, mcmc_p = mcmc(N, M, Psi, inv_Psi)\n",
    "\n",
    "print('MV   Pr: {:1.3f}'.format(np.average([x[1] for x in mv_p])))\n",
    "print('EM   Pr: {:1.3f}\\tlog-likelihood: {:3.2f}\\tA: {}\\tp: {}'.format(np.average([x[1] for x in em_p]), log_likelihood(Psi, em_A, em_p), em_A, em_p))\n",
    "print('RND  Pr: {:1.3f}\\tlog-likelihood: {:3.2f}\\tA: {}\\tp: {}'.format(np.average([x[1] for x in bf_p]), log_likelihood(Psi, bf_A, bf_p), bf_A, bf_p))\n",
    "print('MCMC Pr: {:1.3f}\\tlog-likelihood: {:3.2f}\\tA: {}\\tp: {}'.format(np.average([x[1] for x in mcmc_p]), log_likelihood(Psi, mcmc_A, mcmc_p), mcmc_A, mcmc_p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}