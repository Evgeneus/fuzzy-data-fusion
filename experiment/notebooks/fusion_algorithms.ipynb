{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MV Pr: 0.666666666667\n",
      "EM Pr: 0.993016685642, log-likelihood: -9.33725210422\n",
      "Random Loglikehood Pr: 0.88864298872, log-likelihood: -13.0462257192\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "\n",
    "def MV(Psi):\n",
    "    \"\"\"\n",
    "    The majority voting method.\n",
    "    \"\"\"\n",
    "    return [np.bincount([y[1] for y in x])/(0.0+len(x)) for x in Psi]\n",
    "\n",
    "\n",
    "def EM(N, M, Psi, inv_Psi):\n",
    "    \"\"\"\n",
    "    The expectation maximization method (EM) from Dong et al., 2013.\n",
    "    \"\"\"\n",
    "    # to compute exp of each cell in a matrix\n",
    "    exp = np.vectorize(math.exp)\n",
    "\n",
    "    # convergence eps\n",
    "    eps = 0.01\n",
    "\n",
    "    # init accuracies\n",
    "    A = [0.8]*N\n",
    "    while True:\n",
    "        # accuracy scores\n",
    "        AS = [math.log(x/(1-x)) for x in A]\n",
    "\n",
    "        # confidence of values\n",
    "        C_v = [[0, 0]]*M\n",
    "\n",
    "        # summing accuracy scores of value providers\n",
    "        for obj_id in range(M):\n",
    "            for (source_id, value_id) in Psi[obj_id]:\n",
    "                C_v[obj_id][value_id] += AS[source_id]\n",
    "\n",
    "        # compute probs        \n",
    "        p = [x/sum(x) for x in exp(C_v)]\n",
    "\n",
    "        # update accuracies\n",
    "        A_new = [np.average([p[y[0]][y[1]] for y in x]) for x in inv_Psi]\n",
    "\n",
    "        # convergence check\n",
    "        if sum(np.subtract(A,A_new)) < eps:\n",
    "            break\n",
    "        else:\n",
    "            A = A_new\n",
    "    return A, p\n",
    "\n",
    "\n",
    "def log_likelihood(Psi, A, p):\n",
    "    \"\"\"\n",
    "    Computes the log likelihood of the Psi using A and p.\n",
    "    \"\"\"\n",
    "    res = 0\n",
    "    for obj_id in range(M):\n",
    "        for source_id, value_id in Psi[obj_id]:\n",
    "            if value_id == 1:\n",
    "                res += math.log(A[source_id]*p[obj_id][value_id])\n",
    "            else:\n",
    "                res += math.log((1-A[source_id])*(1-p[obj_id][value_id]))\n",
    "    return res\n",
    "\n",
    "\n",
    "def random_log_likelihood(N, M, Psi):\n",
    "    \"\"\"\n",
    "    Searches for the max log likelihood at random.\n",
    "    \"\"\"\n",
    "    # number of attempts\n",
    "    N_iter = 10000\n",
    "\n",
    "    max_log_likelihood = -100\n",
    "    bf_A = []\n",
    "    bf_p = []\n",
    "    for i in range(N_iter):\n",
    "        A = np.random.uniform(0.8, 1.0, N)\n",
    "        p = [[1-x, x] for x in np.random.uniform(0, 1, M)]\n",
    "        cur_ll = log_likelihood(Psi, A, p)\n",
    "        if cur_ll > max_log_likelihood:\n",
    "            max_log_likelihood = cur_ll\n",
    "            bf_A = A\n",
    "            bf_p = p\n",
    "\n",
    "    return bf_A, bf_p\n",
    "    \n",
    "\n",
    "def mcmc():\n",
    "    \"\"\"\n",
    "    MCMC for log-likelihood maximum search.\n",
    "    \"\"\"\n",
    "    # random init\n",
    "    A = np.random.uniform(0.8, 1.0, N)\n",
    "    p = [[1-x, x] for x in np.random.uniform(0, 1, M)]\n",
    "    \n",
    "    # MCMC sampling\n",
    "    # update sources (this is MAP!)\n",
    "    A = [np.average([p[obj_id][value_id] for obj_id, value_id in inv_Psi[source_id]]) for source_id in range(N)]\n",
    "    # update objects\n",
    "\n",
    "\n",
    "# number of sources\n",
    "N = 3\n",
    "# number of objects\n",
    "M = 5\n",
    "# observations\n",
    "Psi = [[(0, 1), (1, 1), (2, 0)],\n",
    "       [(0, 1), (1, 0), (2, 1)],\n",
    "       [(0, 0), (1, 1), (2, 0)],\n",
    "       [(0, 1), (1, 0), (2, 1)],\n",
    "       [(0, 1), (1, 1), (2, 1)]]\n",
    "\n",
    "# inverted observations\n",
    "inv_Psi = [[(0, 1), (1, 1), (2, 0), (3, 1), (4, 1)],\n",
    "          [(0, 1), (1, 0), (2, 1), (3, 0), (4, 1)],\n",
    "          [(0, 0), (1, 1), (2, 0), (3, 1), (4, 1)]]\n",
    "\n",
    "mv_p = MV(Psi)\n",
    "em_A, em_p = EM(N, M, Psi, inv_Psi)\n",
    "bf_A, bf_p = random_log_likelihood(N, M, Psi)\n",
    "\n",
    "print('MV Pr: {}'.format(np.average([x[1] for x in mv_p])))\n",
    "print('EM Pr: {}, log-likelihood: {}'.format(np.average([x[1] for x in em_p]), log_likelihood(Psi, em_A, em_p)))\n",
    "print('Random Loglikehood Pr: {}, log-likelihood: {}'.format(np.average([x[1] for x in bf_p]), log_likelihood(Psi, bf_A, bf_p)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
